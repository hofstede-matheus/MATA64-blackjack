{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gym[toy_text]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "policy = np.random.randint(env.action_space.n, size=21)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = np.zeros((21 + 1, 2, 21 + 1))\n",
    "\n",
    "for state in range(21):\n",
    "    transition[state, 0, 21] = 1\n",
    "    posible_states = min(21 - state, 11)\n",
    "    if (state > 21 - 11):\n",
    "        transition[state, 1, 21] = (11 - posible_states) / 11\n",
    "    for next_state in range(state + 1, state + posible_states + 1):\n",
    "        transition[state, 1, next_state] = 1 / posible_states\n",
    "\n",
    "# transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = np.zeros(env.observation_space[0].n)\n",
    "\n",
    "# for s in range(env.observation_space[0].n):\n",
    "#     rewards[s] = s if s <= 21 else -1\n",
    "\n",
    "# rewards\n",
    "\n",
    "rewards = np.zeros((21 + 1, 2, 21 + 1))\n",
    "\n",
    "for state in range(21):\n",
    "    rewards[state, 0, 21] = state\n",
    "\n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, reward_function, transition_model, gamma):\n",
    "        self.num_states = transition_model.shape[0]\n",
    "        self.num_actions = transition_model.shape[1]\n",
    "        self.reward_function = np.nan_to_num(reward_function)\n",
    "        self.transition_model = transition_model\n",
    "        self.gamma = gamma\n",
    "        self.values = np.zeros(self.num_states)\n",
    "        self.policy = None\n",
    "\n",
    "    def one_iteration(self):\n",
    "        delta = 0\n",
    "        for s in range(self.num_states):\n",
    "            temp = self.values[s]\n",
    "            v_list = np.zeros(self.num_actions)\n",
    "            for a in range(self.num_actions):\n",
    "                p = self.transition_model[s, a]\n",
    "                v_state = np.zeros(self.num_states)\n",
    "                for next_state in range(len(p)):\n",
    "                    v_state[next_state] = p[next_state] * (self.reward_function[s, a, next_state] + self.gamma * self.values[next_state])\n",
    "                v_list[a] = np.sum(v_state)\n",
    "            self.values[s] = max(v_list)\n",
    "            delta = max(delta, abs(temp - self.values[s]))\n",
    "        return delta\n",
    "\n",
    "    def get_policy(self):\n",
    "        pi = np.ones(self.num_states) * -1\n",
    "        for s in range(self.num_states):\n",
    "            v_list = np.zeros(self.num_actions)\n",
    "            for a in range(self.num_actions):\n",
    "                p = self.transition_model[s, a]\n",
    "                v_state = np.zeros(self.num_states)\n",
    "                for next_state in range(len(p)):\n",
    "                    v_state[next_state] = p[next_state] * (self.reward_function[s, a, next_state] + self.gamma * self.values[next_state])\n",
    "                v_list[a] = np.sum(v_state)\n",
    "\n",
    "            max_index = []\n",
    "            max_val = np.max(v_list)\n",
    "            for a in range(self.num_actions):\n",
    "                if v_list[a] == max_val:\n",
    "                    max_index.append(a)\n",
    "            pi[s] = np.random.choice(max_index)\n",
    "        return pi.astype(int)\n",
    "\n",
    "    def train(self, tol=1e-3, plot=True):\n",
    "        epoch = 0\n",
    "        delta = self.one_iteration()\n",
    "        delta_history = [delta]\n",
    "        while delta > tol:\n",
    "            epoch += 1\n",
    "            delta = self.one_iteration()\n",
    "            delta_history.append(delta)\n",
    "            if delta < tol:\n",
    "                break\n",
    "        self.policy = self.get_policy()\n",
    "        \n",
    "        print(f'# iterations of policy improvement: {len(delta_history)}')\n",
    "        print(f'delta = {delta_history}')\n",
    "        print(self.policy)\n",
    "\n",
    "        if plot is True:\n",
    "            _, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=200)\n",
    "            ax.plot(np.arange(len(delta_history)) + 1, delta_history, marker='o', markersize=4,\n",
    "                    alpha=0.7, color='#2ca02c', label=r'$\\gamma= $' + f'{self.gamma}')\n",
    "            ax.set_xlabel('Iteration')\n",
    "            ax.set_ylabel('Delta')\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "solver = ValueIteration(rewards, transition, gamma=0.9)\n",
    "solver.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
